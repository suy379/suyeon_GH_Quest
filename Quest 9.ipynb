{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quest 9. 유튜브 강의 듣고, 코드 보고 주석 달기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 10. 딥러닝\n",
    "- softmax classification과 cnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#유튜브 강의 참고- \"모두를 위한 딥러닝\"\n",
    "#모두를 위한 딥러닝 LAB 1: tensorflow 설치 및 기본적 operation\n",
    "#tensorflow 다운로드 방법: https://brunch.co.kr/@mapthecity/15 참고\n",
    "\n",
    "#딥러닝이란, 말그대로 기계학습이다. 어떤 이미지가 들어왔을 때 컴퓨터가 자동적으로 분류해주도록 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본\n",
    "import tensorflow as tf\n",
    "hello=tf.constant(\"Hello,Tensorflow!\")\n",
    "sess=tf.Session()\n",
    "print(sess.run(hello)) #'b'가 나오는 것은 그냥 'bite'라는 뜻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow의 매커니즘(꼭 알고있자@@)\n",
    "#1) build graph, using tensorflow operations\n",
    "#2) feed data & run graph(operation) => sess.run(op)\n",
    "#3) update variables in the graph and return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computational graph\n",
    "#1)\n",
    "node1=tf.constant(3.0, tf.float32)\n",
    "node2=tf.constant(4.0)\n",
    "node3=tf.add(node1,node2)\n",
    "#2)\n",
    "sess=tf.Session()\n",
    "#3)\n",
    "print(\"sess.run(node1,node2): \", sess.run([node1,node2]))\n",
    "print(\"sess.run(node3): \", sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Tensor'란 array를 의미.\n",
    "#옵션- rank(몇 차원인지?), shape(개수), type(타입 ex.tf.loat32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-29a98e9e82d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow에서 기본적으로 제공하는 mnist데이터를 받아옵니다. \n",
    "#이 데이터는 사진(28*28*1)으로, 흰색 배경에 검은색으로 0~9까지 숫자가 필기체로 적혀있습니다. \n",
    "#이때 이 사진에 어떤 수가 적혀있는 것인지 맞추게 하는 것이 이번 딥러닝 예제의 목표입니다..!\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax classification-basic NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로우 노드를 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#이때 placeholder는 나중에 데이터를 넣을 수 있는 '통' 같은 개념이라 이해하시면 편합니다. \n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,784],name='x')\n",
    "y=tf.placeholder(tf.float32,[None,10],name='y')\n",
    "    \n",
    "#여기서 784라는 숫자는 사진이 28*28*1이기 때문에 한 사진당 총 픽셀이 28의 제곱인 784개 들어있기 떄문에 사용합니다. \n",
    "#즉 softmax classification 방법은 데이터를 일렬로 쭉 펴서 저장해두는 방식을 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEC 5-1, 5-2: Logistic Classification의 가설함수(H(x)), cost 함수\n",
    "\n",
    "##Linear regression에서는..\n",
    "#가설함수: H(x)=Wx\n",
    "#Cost(가설함수와 실제 데이터 간 편차): cost(W) (가설함수가 직선이므로, cost함수는 2차함수모양)\n",
    "##Cost가 가장 최소화되는 W(가중치)의 값을 찾아야 한다. 그러기 위해 경사하강법 접근\n",
    "#Gradient decent(경사하강법)- 위의 cost함수를 미분하여 0이 되는 지점 찾기!\n",
    "\n",
    "#그런데 이것을 사용하면 binary한 분류(0,1로만 나눠짐)에선 적합하지 않다.\n",
    "##Logistic Classification에서는..\n",
    "#그래서 새롭게 만든 가설함수: H(x)=Wx =z\n",
    "#z를 넣었을 때 값이 0,1로만 나오게 해주는 g(z)를 logistic fuction, sigmoid functiond이라 함=>1/(1+e^-Wx)\n",
    "#그런데 g(z)를 이용해 만든 cost함수는 너무 구불구불한게 많다. 그래서 새로운 cost function 사용\n",
    "#cost funtion: C(H(x),y)=-log(H(X)) (y=1) , -log(1-H(X)) (y=0) \n",
    "##이는 실제 값이 y=1일 때, h(x)=1로 예측된다면 cost=0, h(x)=0예측된다면 cost가 무한대에 가까워지게 하는 매커니즘이다.\n",
    "##즉, 잘못 예측시 패널티를 아주 크게 주는 것! y=0일때도 마찬가지다.(대입해서 생각해보자)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가설함수 H(x)=Wx+b를 만들어 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer1\"): #layer:층의 개념\n",
    "    #우리가 글로 적을 때는 Wx+b라 적지만 실제 코딩을 할떄는 xW+b와 같은 형태입니다.\n",
    "    #x가 n*784, W가 784*28 b가 n*28이고 따라서 layer1은 n*28이 될 것입니다. \n",
    "    W1=tf.Variable(tf.random_normal([784,28]),name='weight1') #random_normal 랜덤배정\n",
    "    b1=tf.Variable(tf.random_normal([28]),name='bias1')\n",
    "    layer1=tf.sigmoid(tf.matmul(x,W1)+b1) #sigmoid함수이용=>결과를 0~1의 값으로 반환\n",
    "    \n",
    "    \n",
    "    #summary.histogram은 tensorboard에서 표를 그릴때 사용하는 것으로 \n",
    "    #나중에 tensorboard를 배우고 싶으신 분들이 있을까봐 참고용으로 지우지 않고 넣어두었습니다. \n",
    "    #w1_hist=tf.summary.histogram(\"weighth1\",W1)\n",
    "    #b1_hist=tf.summary.histogram(\"biash1\",b1)\n",
    "    #layer1_hist=tf.summary.histogram(\"layer1\",layer1)\n",
    "\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2=tf.Variable(tf.random_normal([28,10]),name='weight2')\n",
    "    b2=tf.Variable(tf.random_normal([10]),name='bias2')\n",
    "    \n",
    "    #logits는 Wx+b로 우리가 W,b를 조절하며 계산,예측한 값을 의미합니다. \n",
    "    logits=tf.matmul(layer1,W2)+b2\n",
    "    hypo=tf.nn.softmax(logits) #softmax함수이용=>score를 probability로 바꿈(더해서 1이되도록.)\n",
    "   \n",
    "    #w2_hist=tf.summary.histogram(\"weighth2\",W2)\n",
    "    #b2_hist=tf.summary.histogram(\"biash2\",b2)\n",
    "    #logits_hist=tf.summary.histogram(\"logits\",logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost함수를 만들어 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"cost\"):\n",
    "    cost=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=y)\n",
    "    #cost_sum=tf.summary.scalar(\"cost\",cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost를 줄이는 optimizer로 우리가 잘 아는 경사하강법(gradient descent)을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 우리가 만든 가설함수의 정확성을 확인하기 위한 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction=tf.argmax(hypo,axis=1)\n",
    "\n",
    "is_correct=tf.equal(prediction,tf.argmax(y,1))\n",
    "#여기서 주의할 점은 뒤에 실행 코드를 보시면 아시겠지만 이 x,y에는 train이 아닌 test데이터를 집어넣습니다.  \n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "#accuracy_sum=tf.summary.scalar(\"accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 지금까지는 가설함수와 층을 만들고, 코스트를 줄이는 방식(여기서는 경사하강법)을 정하는 등 텐서플로우 상에서 'graph를 그린 것'입니다. \n",
    "### 이제부터 할 것은 실제로 그 안에서 [가설함수의 코스트를 줄이는 방향]으로 학습을 진행하라는 코드를 볼 것입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #이는 sess=tf.Session()과 같은 의미로 계산 과정을 시작하자는 시동을 거는 것입니다. \n",
    "    \n",
    "    #parameters로 iter_epoch는 전체 학습 반복 횟수, batch_size는 한번에 읽어들여서 학습시키는 양을 의미합니다. \n",
    "    #batch_size는 왜 필요할까요?\n",
    "    iter_epoch=15\n",
    "    batch_size=100\n",
    "    \n",
    "    \n",
    "    #global_step=0\n",
    "    #merged=tf.summary.merge([accuracy_sum,cost_sum])\n",
    "    #writer=tf.summary.FileWriter('c:\\\\GH\\\\tensor')\n",
    "    #writer.add_graph(sess.graph)\n",
    "    #valid_x=mnist.validation.images\n",
    "    #valid_y=mnist.validation.labels\n",
    "    \n",
    "    \n",
    "    #전역 변수 initializer을 사용하여 우리가 만든 W등의 초기화를 미리 지정한 방식(이 경우 random_normal)으로 초기화해줍니다. \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(iter_epoch):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)\n",
    "        #total_batch는 전체 train 데이터 개수를 미리 설정한 batch_size(이 경우 100)으로 나눈 값으로 \n",
    "        #전체 train을 1번 완료 하려면 batch를 100개씩 넣는 작업을 몇번 반복해야하는 것인지를 뜻합니다 \n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
    "            #train.next_batch는 다음 batch만큼의 데이터를 가져와서 batch_x,batch_y에 넣습니다.\n",
    "            #이때 물론 batch_y는 각각의 사진의 실제 답,label입니다. \n",
    "            \n",
    "            \n",
    "            c,s,_=sess.run([cost,accuracy,optimizer],feed_dict={x:batch_x,y:batch_y})\n",
    "            #cost, accuracy_sum에 batch만큼의 데이터를 집어넣어 줍니다. 이때 동시에 optimzer에도 넣어 cost를 줄여주는 학습을 진행합니다.\n",
    "            #optimzer가 나타내는 값 자체는 중요하지 않기 때문에 _ 변수에 저장해주고 이런 _변수는 보통 활용하지 않을 변수에 사용합니다. \n",
    "            \n",
    "            \n",
    "            avg_cost+=c/total_batch\n",
    "            #평균 비용은 각 cost를 반복횟수인 train_batch로 나누어서 구합니다.\n",
    "            \n",
    "            \n",
    "            #s=sess.run(merged,feed_dict={x:valid_x,y:valid_y})  \n",
    "            #writer.add_summary(s,global_step)  \n",
    "            #global_step+=1\n",
    "      \n",
    "       # print('Epoch:','%d' %(epoch+1), 'cost=','{0}'.format(avg_cost))\n",
    "        print(\"Accuracy\",accuracy.eval(session=sess,feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "        \n",
    "        r = random.randint(0, mnist.test.num_examples - 1)\n",
    "        print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "        print(\"Prediction: \", sess.run(\n",
    "            tf.argmax(logits, 1), feed_dict={x: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "        plt.imshow(mnist.test.images[r:r + 1].\n",
    "                   reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # reproducibility를 위해 지정해둡니다. \n",
    "\n",
    "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout (keep_prob) rate  0.7~0.5 가 train시 권장되고 test 시에는 1을 사용해야합니다. 이 부분은 뒤에서 ppt와 함께 다시 설명해드리겠습니다. \n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 기본 노드들을 만들어봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax 코드 부분과 다른점이 뭘까요..!? X_img\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (흑백 사진이기에, 컬러였으면 RGB로 28*28*3 이었을 것입니다)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn 필터들과 다층 layer를 만들어봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "#필터의 개별 크기는 [3,3,1(이는 흑백이어서,컬러면 3)]인 것이고 필터의 총 개수가 32개인것입니다. \n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob) \n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''\n",
    "\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "'''\n",
    "\n",
    "# L3 ImgIn shape=(?, 7, 7, 64)\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "#    Conv      ->(?, 7, 7, 128)\n",
    "#    Pool      ->(?, 4, 4, 128)\n",
    "#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
    "                    1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "#엇 왜 애써 사진 모양처럼 했던 것을 다시 flat하게 펼까요? \n",
    "'''\n",
    "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn 끝 부분에서 위에서 배웠던 softmax을 연결하여 학습시킵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L4 FC 4x4x128 inputs -> 625 outputs\n",
    "#random_normal을 쓰다가 Xavier_initializer을 쓰는 이유는 뭘까요?  \n",
    "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
    "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
    "'''\n",
    "# L5 Final FC 625 inputs -> 10 outputs\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "'''\n",
    "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
    "'''\n",
    "#여기서 중요한 것.! 결국 최종적으로 마지막 logits은 10개의 라벨(0~9까지의 수)\n",
    "#각각으로 예상할 확률로 만들어졌다는 것. ex: N(데이터 개수) * [1 2 3 2 5 1 0 1 2 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "#AdamOptimizer는 왜 사용할까요 ?\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 본격적인 학습을 시작합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분은 학습하는데 시간이 오래 걸립니다. \n",
    "\n",
    "sess = tf.Session()\n",
    "#sess = tf.Session()와  위에서 쓴 with tf.Session() as sess는 동일한 의미입니다. 단, with - as : 방법은 \n",
    "#들여쓰기를 한 부분까지만 Session이 유지되고 그 후에는 Session이 닫힙니다. \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 내 모델의 정확도를 측정해보고 그림을 그려서 확인해봅시다. \n",
    "\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
